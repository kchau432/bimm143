---
title: "Machine Learning 2"
author: "Katie Chau"
date: "2/14/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preparing the Data 

Read the CSV and store the data.

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data,row.names=1)
wisc.df
```
Create a new dataframe that omits the first column 'diagnosis' and make a separate new vector for just the diagnosis data. 
```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```
>Q1. How many observations are in this dataset 

```{r}
dim(wisc.df)
```
There are 31 observations in this dataset. 

>Q2. How many of the observations have a malignant diagnosis?

```{r}
malig <- grep("M",diagnosis)
```
```{r}
length(malig)
```

There are 212 malignant diagnosis. 

>Q3. How many variables/features in the data are suffixed with _mean? 

```{r}
grep("_mean", names(wisc.df), value=TRUE) 
```
10 variables/features in the data are suffixed with _mean. 

## Principal Component Analysis

Check the mean and standard deviation of the columns of wisc.data to determine if scaling is needed. 
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)

```

```{r}
wisc.pr <- prcomp(scale(wisc.data))
summary(wisc.pr)
```

>Q4 From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 proportion of variance for PC1.

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PC's

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PC's

# Interpreting PCA results

Create a biplot of wisc.pr 
```{r}
biplot(wisc.pr)
```


>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

It is very difficult to understand since there are so many characters on the plot rather than dots. The plot is too small for all the data and the data is plotted as values. The rownames are used for the plotting character. 

Generate a scatter plot of each observation along PC1 and PC2 

```{r}

plot( wisc.pr$x, col= ifelse(diagnosis=="M","red","black") , 
     xlab = "PC1", ylab = "PC2")

```

```{r}
plot(wisc.pr$x[,1-3], col = ifelse(diagnosis=="M","red","black"), 
     xlab = "PC1", ylab = "PC3")
```

>Q8 Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

PC2 vs PC1 has more separation between the groups while PC3 and PC1 are more intermixed. 

# Use ggplot2 to make a fancier figure 

```{r}
df<- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

library(ggplot2)

ggplot(df)+aes(PC1,PC2,col=diagnosis)+geom_point() 
```
# Variance Explained 
Create a scree plot to show the proportion of variance as the number of principal components increase. 
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
# Communicating PCA Results 

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```

The component of loading vector for concave.points_mean is -0.26085376

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimum number of principal components required for 80% variance is 5. 


## Hierarchical Clustering 

Scale the wisc.data using scale() and calculate the (Euclidean) distance between all pairs of observations in the new scaled dataset. 
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method = "complete", members = NULL)
```


>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?


```{r}
plot(wisc.hclust)
abline(wisc.hclust,col="red", lty =2)
```

The height is 19. 

# Selecting number of clusters 

Compare outputs from hierarchail clustering model to actual diagnoses. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters,diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 2)
table(wisc.hclust.clusters,diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 7)
table(wisc.hclust.clusters,diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 10)
table(wisc.hclust.clusters,diagnosis)
```

If you cut into 2 clusters, the clusters are more equal. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters,diagnosis)
```

# Using different methods 

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.


```{r}
x <- hclust(data.dist,"single")
plot(x)

```


```{r}
x <- hclust(data.dist,"complete")
plot(x)

```

```{r}
x <- hclust(data.dist,"average")
plot(x)

```

```{r}
x <- hclust(data.dist,"ward.D2")
plot(x)

```

My favorite is ward.D2 since the data is more equally spread out on each side. 


## Combining methods 
#Clustering on PCA results 

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method ="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col= ifelse(diagnosis=="M","red","black"))
```
turn our groups into a factor and reorder the levels. 

```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```
Compare the new hierarchical clustering model with the actual diagnosis. 
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters,diagnosis)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

It is the same. 



k-means clustering
> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?


```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
table(wisc.km$cluster,diagnosis)
```


```{r}
table( wisc.hclust.clusters,wisc.km$cluster)
```

>Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.


```{r}
table(wisc.pr.hclust.clusters, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

They are similar. 

## Sensitivity / Specificity  


>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

```{r}
#for k=1, this is a predominantly malignant cluster that can be used to determine sensitivity.
y <- 188/212
y
```
```{r}
#for k=2, this is a predominantly benign cluster and can be used to determine specificity.
z<- 329/353
z
```


## Prediction 

Project data from the PCA model before and new cancer cell data onto PCA space using the predict() function. 

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18. Which of these new patients should we prioritize for follow up based on your results?

My results are not the same as the class document, but based on the plot in the class document, we should prioritize patient 2 since it is more in the red where the data is more malignant. 
